{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c29b04-9b6c-4e8b-ae7d-e4e56b4e6bda",
   "metadata": {},
   "source": [
    "### In this notebook we will assume we have already downloaded the data from kaggle and put it into a standard structured format.\n",
    "\n",
    "### The purpose of this notebook is to display the functionality of later parts of the ingestion pipeline, including the chunker and the embedding model. \n",
    "\n",
    "### Note - you will need to set up the conda environment in order to run this notebook.  See the readme for more details.\n",
    "\n",
    "The first step is to initialize a chunker and an embedding model.  The chunker will either chunk by sentences or my number of tokens.  Here we chunk by sentences.  The Embedding model is mainly wrapper for Huggingface Sentence Transformers - it handles passing batches of passages to the Sentence Transformer model. We can use any Sentence Transformer, here we use 'all-MiniLM-L6-v2' a lightweight model that will allow us reasonable throughput without a GPU.  Note we can use a very large batch size as long as we keep the chunks reasonably short since the embedding model's memory usage scales with sequence length squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7edd9f-2ce8-4af4-99d0-ab5ea51544d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/williamshabecoff/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading all-MiniLM-L6-v2 from sentence transformers.\n",
      "Model max sequence length is 256.\n"
     ]
    }
   ],
   "source": [
    "from Chunker.Chunking import Chunker\n",
    "from Embed.Embeddings import EmbeddingModel\n",
    "\n",
    "chunker = Chunker(chunk_type='sentence', max_len = 64)\n",
    "model = EmbeddingModel(model_name = 'all-MiniLM-L6-v2', batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd135aea-bebc-494b-a1d9-588af878225b",
   "metadata": {},
   "source": [
    "### Now we initialize a TextDataset object\n",
    "\n",
    "We will not chunk on initialization since we will chunk again when computing the embeddings. TextDataset keeps track of documents and chunks. Embeddings are stored seperately as a compressed numpy array (npz file) - but we save the mappings between embeddings and chunks in the TextDataset.\n",
    "\n",
    "So you don't have to wait long on the embeddings - I have set aside a sample set of 2000 of the football articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5b9a2b-20a4-4e31-8316-03424b2d76d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents:  88%|████████████████████████████████████████████▋      | 1751/2000 [00:44<00:05, 42.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking documents: 100%|███████████████████████████████████████████████████| 2000/2000 [00:50<00:00, 39.25it/s]\n",
      "Embedding passages: 100%|█████████████████████████████████████████████████████| 153/153 [01:57<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from TextDataset.utils import make_dataset, chunk_and_embed_dataset\n",
    "\n",
    "football_ds = make_dataset('Data/Text/football-articles-sample', chunker = chunker, save_chunks = False)\n",
    "\n",
    "embs_path = 'Embeddings/football.npz'\n",
    "\n",
    "chunk_and_embed_dataset(\n",
    "    text_dataset=football_ds,\n",
    "    chunker=chunker,\n",
    "    embedding_model=model,\n",
    "    embeddings_path=embs_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04636c5-ba62-4151-b4a0-da24a9af0aa0",
   "metadata": {},
   "source": [
    "### Next we save the TextDataset to json files at the specified directory.  We then load in the dataset as well as the embeddings.\n",
    "\n",
    "We could just use the existing textdataset, but I wanted to show that saving and reloading the object is very easy! This will create json objects in the 'Data/Dataset' directory.  Embeddings are saved to the Embeddings dir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66dadb9c-042d-49c2-bcab-26d604200e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextDataset.utils import save_text_dataset, load_text_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset_pth = 'Data/Dataset/football-articles-sample'\n",
    "\n",
    "save_text_dataset(football_ds, dataset_pth)\n",
    "loaded_ds = load_text_dataset(directory_path = dataset_pth, chunker = chunker)\n",
    "\n",
    "embs = np.load(embs_path)['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac71422-979c-4785-a1cc-7c2c60595155",
   "metadata": {},
   "source": [
    "### Bonus feature! basic similarity search using our embeddings\n",
    "\n",
    "VSS class uses Faiss-based vector similarity search over our chunk embeddings. We can use this to find most similar chunks to an arbitrary query!\n",
    "\n",
    "Our chunks are currently sentences because of our choice of chunker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999a9c58-832b-4987-9eeb-fe990ff6da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from App.Search import VSS\n",
    "\n",
    "engine = VSS(loaded_ds, model, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1444a654-4d63-4df4-8299-0b4692802917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding passages: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Manchester United are one of the most decorated clubs in the history of English football.',\n",
       " 'THE BIGGER PICTURE: Manchester United’s last trophy triumph came in the Europa League back in 2016-17, when Mourinho was at their helm.',\n",
       " '“It is only fitting that Sergio has been recognised with a statue of his own, in celebration and honour of his accomplishments in one of the most important chapters of Manchester City’s rich and long history.” When Sergio Aguero won Manchester City the title with this 😍The greatest Premier League moment of all time?',\n",
       " 'Manchester City secured the 2021-22 Premier League title with a dramatic 3-2 comeback victory against Steven Gerrard’s Aston Villa on the final day of the season, beating Liverpool to glory by a single point.',\n",
       " 'Manchester United are by far the most successful club of the Premier League era, having won 13 league titles since 1992.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Manchester United\\'s greatest win'\n",
    "retrieved_chunks = engine.similarity_search(query)\n",
    "retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4ca18-2e3f-4421-ad67-55527986629c",
   "metadata": {},
   "source": [
    "Results seem quite reasonable, especially impressive for such a lightweight embedding model — with that we have a basic search program built on our data ingestion pipeline!  Feel free to play around with your own football related queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (text-processing-env)",
   "language": "python",
   "name": "text-processing-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
